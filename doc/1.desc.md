
# O que acontece quando chega uma requisição no gateway

Quando uma requisição chega no gateway (porta ```LISTEN_ADDR```, ex ```:8080```), o fluxo é este:

* 1) ```http.Server``` **recebe a conexão e chama o** ```Handler```
     * Isso está montado em ```main.go```.
     * O ```Handler``` final é uma “pilha” (chain) de middlewares envolvendo o reverse proxy.

* 2) Ordem dos middlewares (muito importante)
     * O gateway monta assim:
          1. ```proxy``` (o reverse proxy) é o handler “do centro”.
          2. ```ConcurrencyMiddleware(...)``` envolve o proxy.
          3. Se ```RATE_ENABLED=true```, o ```ratelimit.Middleware(...)``` envolve tudo.
     * Em termos de execução: o último que envolve é o primeiro a rodar.
          * Então **primeiro roda o Rate Limit**, depois **Concurrency**, e por fim o **proxy**.
* 3) **Rate limit decide se pode prosseguir**
     * Implementado em ```middleware.go```.
     * Passos:
          * Calcula uma **chave** para o cliente (por padrão IP do ```RemoteAddr```; ou pelo header ```RATE_KEY_HEADER```; ou ```X-Forwarded-For``` se ```TRUST_XFF=true```).
          * Chama o caso de uso: ```application.Service.Decide(key)``` (```service.go```).
               * Esse service pede um limiter por chave no store (infra): ```store.go```.
               * O limiter usa token-bucket (```x/time/rate```) e responde ```Allow()``` ou não.
          * Se **bloquear**:
               * Responde **429 Too Many Requests**
               * Coloca ```Retry-After``` (em segundos).
               * (Opcional) pode adicionar headers ```X-RateLimit-*``` se ```ADD_RATELIMIT_HEADERS=true```.
          * Se **permitir**: chama o próximo handler (concorrência).

* 4) **Concurrency limit controla quantas requisições simultâneas entram**
     * Implementado em ```concurrency.go```.
     * Passos:
          * Tenta “adquirir uma vaga” no ```ConcurrencyService.Acquire(r.Context())``` (```concurrency_service.go```).
          * O pool real é um semáforo (```chan```) criado por ```chanpool.go```.
          * Se não conseguir vaga dentro de ```CONCURRENCY_TIMEOUT```:
               * Responde ```503 Service Unavailable``` (ou o status configurado).
          * Se conseguir:
               * Faz ```defer release()``` para liberar a vaga no final da request
               * Chama o próximo handler (proxy)

* 5) **Reverse proxy encaminha para o upstream**
     * O handler central é ```httputil.NewSingleHostReverseProxy(target)```.
     * Ele pega a request original e repassa para ```UPSTREAM_URL``` (ex ```http://localhost:8081```).
     * Se o upstream falhar, ```ErrorHandler``` devolve ```502 Bad Gateway```.

Resumo em uma linha:

     * Chegou request → (RateLimit: 429?) → (Concurrency: 503?) → (Proxy: encaminha pro upstream / 502 se der ruim) → resposta volta pro cliente.

//// Se você quiser, eu explico também como RATE_RPS + RATE_BURST se traduzem em “quantas requests passam” 
     na prática (com exemplos tipo RATE_RPS=10/RATE_BURST=20 vs 0.02/1).