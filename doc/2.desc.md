
# Como o rate limit calcula allow/deny

O “allow/deny” do rate limit aqui é um **token bucket** (balde de tokens), usando ```golang.org/x/time/rate```.

Fluxo exato:

* 1. O middleware HTTP calcula uma **chave** (ex: IP do cliente ou header):
     * ```middleware.go``` chama ```DefaultKeyFunc(...)``` em ```middleware.go``` (mesmo arquivo).

     * Resultado: uma string tipo ```"10.0.0.9"``` ou ```"minha-api-key"```.

* 2. Com essa chave, ele pega um limiter no store:
     * ```opts.Store.Get(key)``` onde ```Store``` hoje é infra: ```store.go```
     * O store mantém um ```map[chave]*rate.Limiter```. Ou seja: **cada chave tem seu próprio balde**.

* 3. O limiter decide:
     * ```lim.Allow()```:
          * **true** ⇒ permite a request
          * **false** ⇒ bloqueia a request

Como ```Allow()``` funciona (token bucket):

* O limiter tem um balde com capacidade ```RATE_BURST``` tokens.
* Ele “recarrega” tokens ao longo do tempo a uma taxa RATE_RPS tokens por segundo.
* Cada request consome 1 token imediatamente.
* Então:
     * Se há token disponível → ```Allow()``` = true.
     * Se não há token → ```Allow()``` = false.

Por isso o RATE_BURST muda MUITO a percepção:

* Exemplo A: ```RATE_RPS=0.02``` e ```RATE_BURST=20```
     * Você pode fazer **~20 requests rápidas** e todas dão **200**, porque o balde começa cheio com 20 tokens.
     * Depois disso, ele só recarrega **1 token a cada 50 segundos**, então começa a dar **429** na maioria.

* Exemplo B: ```RATE_RPS=0.02``` e ```RATE_BURST=1```
     * A 1ª passa (200).
     * A 2ª logo em seguida já não tem token → **429**.

Onde vira 429:

* Quando ```Allow()``` retorna false, o middleware responde ```RejectStatus``` (por padrão 429) e coloca ```Retry-After```
(configurado no gateway via ```RETRY_AFTER```).

//// Se quiser, eu monto um mini “simulador mental” com tempos (tipo: requests em t=0s, 1s, 2s…) 
     pra você prever exatamente quando vai dar 200 vs 429 em qualquer RATE_RPS/RATE_BURST.

